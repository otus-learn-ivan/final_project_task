# MapReduce Homework

Это задание на работу с кучей разных инструментов. Надеюсь, каждый из учащихся найдёт в этом задании что-то новое для себя.

Вы научитесь (обязательная часть для проверки):
 * Вычислять среднее и дисперсию в парадигме map-reduce

А самые смелые научатся также (не обязательно, без проверки):
 * Использовать docker образы
 * Работать с hadoop hdfs и hadoop map reduce

## 1. Мама, я - аналитик (обязательная часть задания, для проверки, C++ будет только в этой части)
Возьмите мою заготовку проекта.

В директории input лежит датасет, который я заранее подчистил от кривых строчек, но если хотите выполнить "задание со звёздочкой", то скачайте оригинал датасета с kaggle, ссылка лежит там же рядом.

Изучите написанный мною мейкфайл, он поможет вам скомпилировать ваш код. Чтобы собрать mapper наберите в консоли `make bin/mapper`, аналогично для reducer'а. `make all` соберёт сразу оба файла, а `make clean` приберётся после работы. 

mapper.cpp и reducer.cpp - это заготовки для вашего кода

run_locally.sh поможет вручную потестить код (ручное тестирование не отменяет необходимость unit-тестов)

Это лишь заготовка проекта. Вы можете модицировать любые скрипты при необходимости, делайте как вам удобно. Можете воспользоваться другими инструментами сборки.

За работу... Изучите датасет, если будет непонятно, где какой столбец - почитайте описание на кэггле (https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data?resource=download). Сегодня наша задача - посчитать в парадигме map-reduce пару статистик: среднюю цену на недвижимость в NY и дисперсию цены. Напишите mapper'ы и reducer'ы для этого. Для каждой статистики у вас получится своя пара mapper+reducer. Они должны успешно отрабатывать при запуске run_locally.sh

Если вы не уверены, что посчитали значения правильно, воспользуйтесь вашим любимым инструментом для анализа данных (matlab/R/python+pandas/etc) и проверьте себя. А так же не пренебрегайте юнит-тестами. Самые любознательные могут попробовать выполнить эту работу в технике TDD (https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B7%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D1%87%D0%B5%D1%80%D0%B5%D0%B7_%D1%82%D0%B5%D1%81%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5), ведь учёба - отличное время, чтобы пробовать новые подходы. 

К концу этого этапа у вас будет две пары mapper+reducer, которые умеют вычислять среднее и дисперсию цены из данного вам датасета. Сдайте их на проверку вместе со всем, что нужно для сборки и проверки.

## 2. Что общего между devops и бездомным? Оба хорошо разбираются в контейнерах (необязательная часть; не для проверки; для тех, кто осилит)
Итак, если вы ещё не умеете пользоваться docker, то сейчас - самое время научиться. Вам помогут `man docker` и интернет (https://habr.com/ru/companies/ruvds/articles/438796/).  
Docker - замечательный инструмент, это такая виртуалка уровня ОС, но не уровня железа. Используется, чтобы поставлять ваше ПО вместе с необходимым окружением, быстро разворачивать это на любом сервере или на куче серверов сразу. Он обязательно пригодится вам в жизни. Сейчас мы не будем учиться упаковывать в образ своё ПО, а будем только использовать чужие образы.  
Этот этап у пользователей windows может проходить больнее, чем у unixоидов, таков путь.

И ещё один инструмент - docker-compose. Он бывает полезен, когда нужно запустить сразу несколько docker-контейнеров в связке. Аналогично: `man docker-compose` и habr (https://habr.com/ru/companies/ruvds/articles/450312/).

Теперь мы готовы к продолжению...

Берём чужую репу https://github.com/tech4242/docker-hadoop-hive-parquet, клонируем и выполняем `docker-compose up`. У нас запускается несколько контейнеров с разными частями hadoop'а, список можно посмотреть в `docker ps` (первая колонка - id контейнера). Скопировать файлы со своей машины в любой из запущенных контейнеров можно с помощью `docker cp <src_file> <cont_id>:<dest_path>`. Открыть консольку в любом из контейнеров `docker exec -it <id> /bin/bash`. 

Сейчас наша задача - добиться, чтобы ваши mapper и reducer умели запускаться в контейнере datanode. Скопируйте их туда, откройте консольку и попробуйте запустить. Скорее всего, версии libc на вашей машине, где вы компилировали бинари, и в контейнере не совпадут, может вылезут и ещё какие-нибудь проблемы. Доблестно решаем проблемы, кто справится - тот молодец и сможет перейти к следующему этапу задания. Hint: подозреваю, что самый короткий путь - взять ещё один docker-образ с такой же версией оперционки, как в уже запущенных контейнерах, и с установленным g++, скомпилить ваши бинари в этой среде, тогда они запустятся и в целевых контейнерах, т.е. нужно производить сборку и запуск в одинаковой среде. Hint2: другой путь - модифицировать образы из этой репы так, чтобы версия ОС в них совпадала с вашей. В реальной жизни, вы, скорее всего, будете сами писать конфигурации докер-образов для своего ПО и будете иметь два докер-образа с похожей конфигурацией: один для сборки и тестирования вашего сервиса в CI, другой - для выкатки в прод, и у вас не будет проблем с версиями libc, потому что вы это сразу предусмотрите. 

Итог второго этапа: вы умеете запускать докер-контейнеры, а ваши mapper и reducer умеют запускаться внутри них (внутри datanode).

## 3. Открываем мир больших данных (необязательная часть; не для проверки; для тех, кто осилит)

### HDFS
1) Скопируйте датасет с вашей машины в контейнер с namenode - поможет `docker cp`
2) Давайте откроем консоль namenode и потыкаем в консольный интерфейс hdfs (https://gist.github.com/Alexflex/c8c53346b2feed4863956ab633fdd493#%D0%BA%D0%BE%D0%BC%D0%B0%D0%BD%D0%B4%D1%8B-shell). Когда освоитесь, что к чему, скопируйте датасет из файловой системы контейнера в hdfs.

### Hadoop Map-Reduce
1) Скопируйте с вашей машины в контейнер с namenode ваши маппер и редьюсер. Их не нужно помещать в hdfs, просто в ФС контейнера.
2) Запустите map reduce задачу для расчёта средней цены (поищите туториалы по Hadoop MapReduce или просто следуйте подсказкам ниже):
     * Hint1: вам понадобится файл hadoop-streaming.jar. С помощью команды `find / -name 'hadoop-streaming*.jar'` найдите, где в контейнере лежит hadoop-streaming.jar. Запускать командой `yarn jar <path> [params]`
     * Hint2: разберитесь, как этим пользоваться, а если будут сложности, то можете взять за основу мой скрипт:
```
#!/usr/bin/env bash
set -x

HADOOP_STREAMING_JAR=/opt/hadoop-2.7.4/share/hadoop/tools/lib/hadoop-streaming-2.7.4.jar
OUT_DIR=$2

hdfs dfs -rm -r $OUT_DIR
 
yarn jar $HADOOP_STREAMING_JAR \
	-D mapreduce.job.name=$3 \  # любое имя для логов
	-files mapper_mean.py,reducer_mean.py \  # список файлов, которые нужно скопировать с namenode на datanode до запуска map reduce
    -mapper 'python3 mapper_mean.py' \  # команда для запуск mapper'а на datanode узле, здесь пример с пайтоном, у вас будет по-другому
    -reducer 'python3 reducer_mean.py' \  # команда для запуск reducer'а на datanode узле
    -numReduceTasks 1 \  # количество редьюсеров
    -input $1 \  # путь к датасету в hdfs
    -output $OUT_DIR  # путь, куда сложить результат в hdfs

hdfs dfs -cat $OUT_DIR/part-00000

echo $?

```

Итог третьего этапа:  
  - вы потыкали в консольную утилиту hdfs, научились записывать файлы в hdfs и делать другие действия с этой файловой системой;
  - вы запустили настоящую mapreduce задачу в настоящем bigdata фреймворке (Hadoop MapReduce) и оно посчитало нужные статистики

**Profit! Вы молодцы! Так держать!**